{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Transformer implementation in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Self-Attention\n",
    "\n",
    "The intuition behind ``self-attention`` is that averaging token embeddings instead of using a fixed embedding for each token, enables the model to capture how words relate to each other in the input. In practice, said weighted relationships (attention weights) represent the syntactic and contextual structure of the sentence, leading to a more nuanced and rich understanding of the data.\n",
    "\n",
    "The most common way to implement a self-attention layer is based on ``scaled dot-product attention``, which involves:\n",
    "1. ``Linear projection`` of each token embedding into three vectors: ``query (q)``, ``key (k)``, ``value (v)``.\n",
    "2. Compute ``scaled attention scores``: we determine the similary between ``q`` and ``k`` by applying the dot product. Since the results of this function are typically large numbers, they are then multiplied by a scaling factor inferred from the dimensionality of (k).\n",
    "3. Normalize the ``attention scores`` into ``attention weights`` by applying softmax (this ensures all the values sum to 1).\n",
    "4. ``Update the token embeddings`` by multiplying the attention weights by the value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        # step 1: linear projection of tokens embeddings into query, key, and value vectors\n",
    "        #\n",
    "        # nn.Linear(in_features, out_features, bias=True) creates a linear transformation.\n",
    "        # self.x is an instance of nn.Linear, which allows the transformation to be applied later\n",
    "        # by calling self.x(input).\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def scaled_dot_product_attention_1(q, k, v):\n",
    "        # step 2: calculate the scaled attention scores\n",
    "        # torch.bmm performs a batch matrix-matrix product of matrices stored in q and k.\n",
    "        # we then apply the scaling factor 1/sqrt(k_dim) to the dot product of q and k.\n",
    "        scaled_attention_logits = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(k.size(-1))\n",
    "        # step 3: apply a softmax function to obtain the attention weights\n",
    "        attention_weights = torch.softmax(scaled_attention_logits, axis=-1)\n",
    "        # step 4: update the token embeddings by applying the attention weights to the value vectors\n",
    "        return torch.bmm(attention_weights, v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_outputs = self.scaled_dot_product_attention(self.q(x), self.k(x), self.v(x))\n",
    "        return attn_outputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

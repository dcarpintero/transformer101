{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Transformer implementation in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Self-Attention\n",
    "\n",
    "The intuition behind ``self-attention`` is that averaging token embeddings instead of using a fixed embedding for each token, enables the model to capture how words relate to each other in the input. In practice, said weighted relationships (attention weights) represent the syntactic and contextual structure of the sentence, leading to a more nuanced and rich understanding of the data.\n",
    "\n",
    "The most common way to implement a self-attention layer is based on ``scaled dot-product attention``, which involves:\n",
    "1. ``Linear projection`` of each token embedding into three vectors: ``query (q)``, ``key (k)``, ``value (v)``.\n",
    "2. Compute ``scaled attention scores``: we determine the similary between ``q`` and ``k`` by applying the dot product. Since the results of this function are typically large numbers, they are then multiplied by a scaling factor inferred from the dimensionality of (k).\n",
    "3. Normalize the ``attention scores`` into ``attention weights`` by applying softmax (this ensures all the values sum to 1).\n",
    "4. ``Update the token embeddings`` by multiplying the attention weights by the value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 1: linear projection of tokens embeddings into query, key, and value vectors\n",
    "        # nn.Linear(in_features, out_features, bias=True) creates a linear transformation.\n",
    "        # self.x is an instance of nn.Linear, this allows the transformation to be applied \n",
    "        # by calling self.x(input).\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        dim_k = torch.tensor(k.size(-1), dtype=torch.float32)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 2: calculate the scaled attention scores\n",
    "        # torch.bmm performs a batch matrix-matrix product of q and k.\n",
    "        # we then apply the scaling factor 1/sqrt(k_dim) to said dot product.\n",
    "        scaled_attention_scores = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(dim_k)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 3: apply a softmax function to obtain the attention weights\n",
    "        attention_weights = torch.softmax(scaled_attention_scores, axis=-1)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 4: update tokens embeddings by applying attention weights to the value vector\n",
    "        output = torch.bmm(attention_weights, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_outputs = self.scaled_dot_product_attention(self.q(x), self.k(x), self.v(x))\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-Headed Attention\n",
    "\n",
    "In a standard attention mechanism, the softmax of a single head tends to concentrate on a specific aspect of similarity, potentially overlooking other relevant features in the input. By integrating multiple attention heads, the model gains the ability to simultaneously attend to various aspects of the input data such as:\n",
    "- semantic meaning of words\n",
    "- grammatical relationships\n",
    "- tone or sentiment\n",
    "- intended modality\n",
    "- idiomatic expressions\n",
    "- [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 1: initialize the attention heads\n",
    "        # E.g. BERT has 12 attention heads whereas the embeddings dimension is 768 \n",
    "        # resulting in 786/12 = 64 as the head dimension\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 2: prepare linear transformation\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 3: concatenate attention heads\n",
    "        attn_outputs = torch.cat([head(hidden_state) for head in self.heads], dim=-1)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 4: linear projection of concatenated attention heads\n",
    "        outputs = self.output_linear(attn_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn = MultiHeadAttention(embed_dim=768, num_heads=12)\n",
    "attn_outputs = multihead_attn(torch.rand(1, 10, 768))\n",
    "attn_outputs.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

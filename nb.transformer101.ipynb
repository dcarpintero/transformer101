{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanilla implementation in Pytorch of the Transformer model as introduced in the paper [Attention Is All You Need, 2017](https://arxiv.org/pdf/1706.03762.pdf) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\n",
    "\n",
    "``Scaled Dot-Product Attention`` | ``Multi-Head Attention`` | ``Absolute Positional Encodings`` | ``Learned Positional Encodings`` | ``Dropout`` | ``Layer Normalization`` | ``Residual Connection`` | ``Linear Layer`` | ``Position-Wise Feed-Forward Layer`` | ``GELU`` | ``Softmax`` | ``Encoder`` | ``Decorder`` | ``Transformer``\n",
    "\n",
    "*Note that this is just an in progress learning project - if you are looking for production grade implementations, refer to the [PyTorch Transformer Class](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html), and [OLMo](https://github.com/allenai/OLMo/), a fully open language model.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "Sequence modeling and transduction tasks, such as language modeling and machine translation, were typically addressed with RNNs and CNNs. However, these architectures are limited by: (i) ``long training times``, due to the sequential nature of RNNs, which constrains parallelization, and results in increased memory and computational demands as the text sequence grows; and (ii) ``difficulty in learning dependencies between distant positions``, where CNNs, although much less sequential than RNNs, require a number of steps to integrate information that is, in most cases, correlated (linearly for models like ConvS2S and logarithmically for ByteNet) with the distance between elements in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Technical Approach\n",
    "\n",
    "The paper 'Attention is All You Need' introduced the novel Transformer model, ``a stacked encoder-decoder architecture that utilizes self-attention mechanisms instead of recurrence and convolution to compute input and output representations``. In this model, each of the six layers of both the encoder and decoder is composed of two main sub-layers: a multihead self-attention sub-layer, which allows the model to focus on different parts of the input sequence, and a position-wise fully connected feed-forward sub-layer.\n",
    "\n",
    "At its core, the ``self-attention mechanism`` enables the model to weight the relationships between input tokens at different positions, resulting in a more effective handling of long-range dependencies. Additionally, by integrating ``multiple attention heads``, the model gains the ability to simultaneously attend to various aspects of the input data during training.\n",
    "\n",
    "In the proposed implementation, the input and output tokens are converted to 512-dimensional embeddings, to which ``positional embeddings`` are added, enabling the model to use sequence order information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transformer Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\" \n",
    "    Transformer (model) configuration\n",
    "    \"\"\"\n",
    "\n",
    "    d_model: int = 768         # dimension of the token embeddings (hideen size of the model)\n",
    "    n_layer: int = 6           # number of encoder/decoder layers\n",
    "    n_head: int = 12           # number of self-attention heads\n",
    "    d_ff: int = 2048           # dimension of the feedforward network\n",
    "    src_vocab_size: int = 32   # size of the source vocabulary\n",
    "    tgt_vocab_size: int = 48   # size of the vocabulary\n",
    "    drop: float = 0.1          # dropout probability\n",
    "    max_seq_len: int = 100     # maximum sequence length\n",
    "    pad_token_id: int = 0      # padding token id (usually 0)\n",
    "    activation: str = \"gelu\"   # activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Self-Attention\n",
    "\n",
    "The intuition behind ``self-attention`` is that averaging token embeddings instead of using a fixed embedding for each token, enables the model to capture how words relate to each other in the input. In practice, said weighted relationships (attention weights) represent the syntactic and contextual structure of the sentence, leading to a more nuanced and rich understanding of the data.\n",
    "\n",
    "The most common way to implement a self-attention layer relies on ``scaled dot-product attention``, and involves:\n",
    "1. ``Linear projection`` of each token embedding into three vectors: ``query (q)``, ``key (k)``, ``value (v)``.\n",
    "2. Compute ``scaled attention scores``: determine the similary between ``q`` and ``k`` by applying the ``dot product``. Since the results of this function are typically large numbers, they are then divided by a scaling factor inferred from the dimensionality of (k). This scaling contributes to stabilize gradients during training.\n",
    "3. Normalize the ``attention scores`` into ``attention weights`` by applying the ``softmax`` function (this ensures all the values sum to 1).\n",
    "4. ``Update the token embeddings`` by multiplying the ``attention weights`` by the ``value vector``.\n",
    "\n",
    "> In addition, the self-attention mechanism of the decoder layer introduces ``masking`` to prevent the decoder from having access to future tokens in the sequence it is generating. In practice, this is implemented with a binary mask that designates which tokens should be attended to (assigned non-zero weights) and which should be ignored (assigned zero weights). In our function, setting the future tokens (upper values) to negative-infinity guarantees that the attention weights become zero after applying the softmax function (e exp -inf == 0). This design aligns with the nature of many tasks like translation, summarization, or text generation, where the output sequence needs to be generated one element at a time, and the prediction of each element should be based only on the previously generated elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single attention head within a multi-head attention mechanism.\n",
    "    \n",
    "    Args:\n",
    "        config (TransformerConfig): The configuration for the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_head = config.d_model // config.n_head\n",
    "\n",
    "        self.q = nn.Linear(config.d_model, self.d_head)\n",
    "        self.k = nn.Linear(config.d_model, self.d_head)\n",
    "        self.v = nn.Linear(config.d_model, self.d_head)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        dim_k = torch.tensor(k.size(-1), dtype=torch.float32)\n",
    "        attn_scores = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(dim_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores, axis=-1)\n",
    "        output = torch.bmm(attn_weights, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q (torch.Tensor): query embeddings.\n",
    "            k (torch.Tensor): key embeddings.\n",
    "            v (torch.Tensor): value embeddings.\n",
    "            mask (torch.Tensor): attention mask.\n",
    "        \"\"\"\n",
    "        output = self.scaled_dot_product_attention(self.q(q), \n",
    "                                                   self.k(k), \n",
    "                                                   self.v(v), \n",
    "                                                   mask=mask)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 64])\n"
     ]
    }
   ],
   "source": [
    "attention_head = AttentionHead(config)\n",
    "\n",
    "x = torch.randn(10, 32, config.d_model)\n",
    "\"\"\"\n",
    "10: batch size\n",
    "32: sequence length\n",
    "config.d_model: hidden size (embedding dimension)\n",
    "\"\"\"\n",
    "\n",
    "output = attention_head(x, x, x)\n",
    "\n",
    "print(output.shape)\n",
    "# Should be [10, 32, d_head == 768 / 12]\n",
    "# \n",
    "# Note that in the linear projection step, q, k, and v are in practice splitted into n_head parts.\n",
    "# Those will be then concatenated and projected to the final output size \n",
    "# in the MultiHeadAttention class (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Multi-Head Attention\n",
    "\n",
    "In a standard attention mechanism, the ``softmax`` of a single head tends to concentrate on a specific aspect of similarity, potentially overlooking other relevant features in the input. By integrating multiple attention heads, the model gains the ability to simultaneously attend to various aspects of the input data.\n",
    "\n",
    "The basic approach to implement Multi-Headed Attention comprises:\n",
    "\n",
    "1. Initialize the ``attention heads``. E.g. BERT has 12 attention heads whereas the embeddings dimension is 768, resulting in 768 / 12 = 64 as the head dimension.\n",
    "2. ``Concatenate attention heads`` to combines the outputs of the attention heads into a single vector while preserving the dimensionality of the embeddings.\n",
    "3. Apply a ``linear projection``.\n",
    "\n",
    "> Note that the softmax function is a probability distribution, which when applied within a single attention head tends to amplify certain features (those with higher scores) while diminishing others. Thus, leading to a focus on specific aspects of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        config (TransformerConfig): The configuration for the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.d_model % config.n_head == 0, \"d_model must be divisible by n_head\"\n",
    "\n",
    "        self.heads = nn.ModuleList([AttentionHead(config) for _ in range(config.n_head)])\n",
    "        self.linear = nn.Linear(config.d_model, config.d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn_outputs = torch.cat([h(q, k, v, mask) for h in self.heads], dim=-1)\n",
    "        output = self.linear(attn_outputs)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 768])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn = MultiHeadAttention(config)\n",
    "\n",
    "x = torch.randn(10, 32, config.d_model)\n",
    "attn_output = multihead_attn(x, x, x)\n",
    "attn_output.size()\n",
    "# Should be [10, 32, d_model\n",
    "# \n",
    "# Note that the output size is the same as the input size, \n",
    "# as the attention scores of each head are concatenated and projected back to the original size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Position-Wise Feed-Forward Layer\n",
    "\n",
    "The Transformer, primarily built upon linear operations like ``dot products`` and ``linear projections``, relies on the ``Position-Wise Feed-Forward Layer`` to introduce non-linearity into the model. This non-linearity enables the model to capture complex data patterns and relationships. The layer typically consists of two linear transformations with a ``non-linear activation function (like ReLU or GELU)``. Each layer in the ``Encoder`` and ``Decoder`` includes one of these feed-forward networks, allowing the model to build increasingly abstract representations of the input data as it passes through successive layers. Note that since this layer processes each embedding independly, the computations can be fully parallelized.\n",
    "\n",
    "In summary, this Layer comprises:\n",
    "- ``First linear transformation`` to the input tensor. \n",
    "- A non-linear ``activation function`` to allow the model learn more complex patterns.\n",
    "- ``Second linear transformation``, increasing the model's capacity to learn complex relationships in the data.\n",
    "- ``Dropout``, a regularization technique used to prevent overfitting. It randomly zeroes some of the elements of the input tensor with a certain probability during training.\n",
    "\n",
    "> Note that the ``ReLU`` function is a faster function that activates units only when the input is possitive, which can lead to sparse activations (that can be intended in some tasks); whereas ``GELU``, introduced after``ReLU``, offers smoother activation by modeling the input as a stochastic process, providing a probabilistic gate in the activation. In practice, ``GELU`` has been the preferred choice in the BERT and GPT models. Although recent models like LLaMA, PaLM, and [OLMo](https://allenai.org/olmo/olmo-paper.pdf) use the [SwiGLU](https://www.semanticscholar.org/paper/GLU-Variants-Improve-Transformer-Shazeer/bdbf780dfd6b3eb0c9e980887feae5f23af15bc4) activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the PositionWiseFeedForward layer.\n",
    "\n",
    "    Args:\n",
    "        config (TransformerConfig): The configuration for the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(config.d_model, config.d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.d_ff, config.d_model),\n",
    "            nn.Dropout(config.drop)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 768])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff = PositionWiseFeedForward(config)\n",
    "x = torch.randn(10, 32, config.d_model)\n",
    "ff(x).size()\n",
    "# Should be [10, 32, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Positional Encoding\n",
    "\n",
    "Since the Transformer model contains no recurrence and no convolution, the model is invariant to the position of the tokens. By adding ``positional encoding`` to the input sequence, the Transformer model can differentiate between tokens based on their position in the sequence, which is important for tasks such as language modeling and machine translation. In practice, ``positional encodings`` are added to the input embeddings at the bottoms of the ``encoder`` and ``decoder`` stacks. \n",
    "\n",
    "> As outlined in the original [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf) paper, ``Sinusoidal Positional Encoding`` and ``Learned Positional Encoding`` produce nearly identical results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the PositionalEncoding layer.\n",
    "\n",
    "    Args:\n",
    "        config (TransformerConfig): The configuration for the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.d_model = config.d_model\n",
    "        self.max_seq_len = config.max_seq_len\n",
    "\n",
    "        position = torch.arange(self.max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.pow(10000, torch.arange(0, self.d_model, 2) / self.d_model)\n",
    "\n",
    "        pe = torch.zeros(1, self.max_seq_len, self.d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position / div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position / div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 64, 768])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pe = PositionalEncoding(config)\n",
    "x = torch.randn(10, 64, config.d_model)\n",
    "pe(x).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Encoder\n",
    "\n",
    "Each of the six layers of both the encoder and decoder is composed of two main sub-layers: a ``multihead self-attention`` sub-layer, which as explained hereinabove allows the model to focus on different parts of the input sequence, and a ``position-wise fully connected feed-forward`` sub-layer. In addition, the model employs a ``residual connection`` around each of the two sub-layers, followed by ``layer normalization``. In our case, we implement pre layer (instead of post layer) normalization with ``Dropout`` regularization to favour stability during training and prevent overfitting, respectively.\n",
    "\n",
    "> ``layer normalization`` contributes to having zero mean and unitity variance. This helps to stabilize the learning process, and to reduce the number of training steps.\n",
    "\n",
    "> ``residual connection`` or ``skip connection`` helps alleaviate the problem of vanishing gradients by passing a tensor to the next layer of the model without processing it and adding it to the processed tensor. In other words, the output of each sub-layer is\n",
    "LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\n",
    "itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Encoder layer.\n",
    "\n",
    "    Args:\n",
    "        config (TransformerConfig): The configuration for the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(config.d_model)\n",
    "        self.masked_attn = MultiHeadAttention(config)\n",
    "\n",
    "        self.norm_2 = nn.LayerNorm(config.d_model)\n",
    "        self.feed_forward = PositionWiseFeedForward(config)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.drop)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_outputs = self.masked_attn(x, x, x, mask=mask)\n",
    "        x = x + self.dropout(attn_outputs)\n",
    "\n",
    "        output = x + self.dropout(self.feed_forward(self.norm_2(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 768])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer = EncoderLayer(config)\n",
    "x = torch.randn(10, 32, config.d_model)\n",
    "encoder_layer(x).size()\n",
    "# Should be [10, 32, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Encoder stack.\n",
    "\n",
    "    Parameters:\n",
    "        config (TransformerConfig): The configuration for the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.src_vocab_size, config.d_model)\n",
    "        self.pe = PositionalEncoding(config)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.n_layer)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(config.d_model)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 768])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(config)\n",
    "x = torch.randint(0, config.src_vocab_size, (10, 32))\n",
    "encoder(x).size()\n",
    "# Should be [10, 32, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Decoder\n",
    "\n",
    "The Decoder has two attention sub-layers: ``masked multi-head self-attention layer`` and ``encoder-decoder attention layer``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements a single Decoder layer.\n",
    "\n",
    "    Args:\n",
    "        config (TransformerConfig): The configuration for the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.norm_1 = nn.LayerNorm(config.d_model)\n",
    "        self.masked_attn = MultiHeadAttention(config)\n",
    "\n",
    "        self.norm_2 = nn.LayerNorm(config.d_model)\n",
    "        self.cross_attn = MultiHeadAttention(config)\n",
    "\n",
    "        self.norm_3 = nn.LayerNorm(config.d_model)\n",
    "        self.feed_forward = PositionWiseFeedForward(config)\n",
    "        \n",
    "        self.dropout = nn.Dropout(config.drop)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        attn_output = self.masked_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm_1(x + self.dropout(attn_output))\n",
    "\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm_2(x + self.dropout(attn_output))\n",
    "\n",
    "        output = self.norm_3(x + self.dropout(self.feed_forward(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 768])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_layer = DecoderLayer(config)\n",
    "x = torch.randn(10, 32, config.d_model)\n",
    "encoder_output = torch.randn(10, 32, config.d_model)\n",
    "decoder_layer(x, encoder_output).size()\n",
    "# Should be [10, 32, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Decoder stack.\n",
    "\n",
    "    Args:\n",
    "        config (TransformerConfig): The configuration for the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(config.tgt_vocab_size, config.d_model)\n",
    "        self.pe = PositionalEncoding(config)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.n_layer)])\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        x = self.embedding(x) * math.sqrt(config.d_model)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 768])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(config)\n",
    "x = torch.randint(0, config.tgt_vocab_size, (10, 32))\n",
    "torch.randn(10, 32, config.d_model).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ``Decoder`` class takes in an additional argument ``enc_output`` which is the output of the ``Encoder`` stack. This is used in the cross-attention mechanism to calculate the attention scores between the decoder input and the encoder output.\n",
    "\n",
    "The ``source mask`` is typically used in the encoder to ignore padding tokens in the input sequence., whereas the ``target mask`` is used in the decoder to ignore also padding tokens, and to ensure that predictions for each token can only depend on previous tokens. This enforces causality in the decoder output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Transformer architecture.\n",
    "\n",
    "    Args:\n",
    "        config (TransformerConfig): The configuration for the transformer model.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "        self.linear = nn.Linear(config.d_model, config.tgt_vocab_size)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != config.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != config.pad_token_id).unsqueeze(1).unsqueeze(3)\n",
    "        tgt_mask = tgt_mask & torch.tril(torch.ones((tgt.size(-1), tgt.size(-1)), device=tgt.device)).bool()\n",
    "      \n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        \n",
    "        enc_output = self.encoder(src, mask=None)\n",
    "        dec_output = self.decoder(tgt, enc_output, src_mask=None, tgt_mask=None)\n",
    "        \n",
    "        return self.linear(dec_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 32, 48])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(config)\n",
    "src = torch.randint(0, config.src_vocab_size, (10, 32))\n",
    "tgt = torch.randint(0, config.tgt_vocab_size, (10, 32))\n",
    "\n",
    "model(src, tgt).size()\n",
    "# Should be [10, 32, tgt_vocab_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock data loader to test if the model can be trained\n",
    "\n",
    "class MockDataLoader:\n",
    "    def __init__(self, batch_size, sequence_length, vocab_size, num_batches):\n",
    "        self.batch_size = batch_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_batches = num_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.num_batches):\n",
    "            src = torch.randint(0, self.vocab_size, (self.batch_size, self.sequence_length))\n",
    "            tgt = torch.randint(0, self.vocab_size, (self.batch_size, self.sequence_length))\n",
    "            yield src, tgt\n",
    "\n",
    "dataloader = MockDataLoader(batch_size=64, sequence_length=32, vocab_size=config.vocab_size, num_batches=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(config)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=config.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "\n",
    "# dataloader provides batches of (src, tgt) pairs\n",
    "for epoch in range(5):\n",
    "    for i, (src, tgt) in enumerate(dataloader):\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = transformer(src, tgt[:, :-1])  # Exclude the last token from the target input to the decoder\n",
    "\n",
    "        # Compute loss\n",
    "        # Shift the target sequences one token to the left\n",
    "        loss = criterion(output.view(-1, config.vocab_size), tgt[:, 1:].contiguous().view(-1))  \n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train our transformer model to translate from english to spanish. The dataset has been obtained from https://tatoeba.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pandas to read txt in tabular pairs\n",
    "import pandas as pd\n",
    "def load_dataset():\n",
    "    df = pd.read_csv('data/en-es.txt', sep='\\t', header=None)\n",
    "    en = df[0].tolist()\n",
    "    es = df[1].tolist()\n",
    "    return en, es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We apologize.',\n",
       " 'We are happy.',\n",
       " 'We are young.',\n",
       " 'We can do it.',\n",
       " \"We can't sue.\",\n",
       " \"We can't win.\",\n",
       " 'We got ready.',\n",
       " 'We got ready.',\n",
       " 'We had lunch.',\n",
       " 'We have some.']"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src[3500:3510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pedimos disculpas.',\n",
       " 'Somos felices.',\n",
       " 'Somos jóvenes.',\n",
       " 'Podemos hacerlo.',\n",
       " 'No podemos demandar.',\n",
       " 'No podemos ganar.',\n",
       " 'Nos preparamos.',\n",
       " 'Estábamos listos.',\n",
       " 'Almorzamos.',\n",
       " 'Tenemos algo.']"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt[3500:3510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118964, 118964)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src), len(tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def create_vocab(corpus):\n",
    "    vocab = set()\n",
    "    for s in corpus:\n",
    "        vocab.update(re.findall(r'\\w+|[^\\w\\s]', s))\n",
    "    w2i = {w: i+4 for i, w in enumerate(vocab)}\n",
    "    w2i['PAD'] = 0\n",
    "    w2i['SOS'] = 1\n",
    "    w2i['EOS'] = 2\n",
    "    w2i['UNK'] = 3\n",
    "    i2w = {i: w for w, i in w2i.items()}\n",
    "\n",
    "    return w2i, i2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14779 28993\n"
     ]
    }
   ],
   "source": [
    "src_w2i, src_i2w = create_vocab(src)\n",
    "tgt_w2i, tgt_i2w = create_vocab(tgt)\n",
    "print(len(src_w2i), len(tgt_w2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(corpus, w2i):\n",
    "    encoding = []\n",
    "    for s in corpus:\n",
    "        s_enc = [w2i[w] for w in re.findall(r'\\w+|[^\\w\\s]', s)]\n",
    "        s_enc = [w2i['SOS']] + s_enc + [w2i['EOS']]\n",
    "        encoding.append(s_enc)\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_enc = encode(src, src_w2i)\n",
    "tgt_enc = encode(tgt, tgt_w2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "src_train, src_test, tgt_train, tgt_test = train_test_split(src_enc,\n",
    "                                                            tgt_enc,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95171, 23793, 95171, 23793)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_train), len(src_test), len(tgt_train), len(tgt_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(X, Y):\n",
    "    max_len_X = max([len(x) for x in X])\n",
    "    max_len_Y = max([len(y) for y in Y])\n",
    "\n",
    "    enc_input = torch.zeros(len(X), max_len_X, dtype=torch.long)\n",
    "    dec_input = torch.zeros(len(Y), max_len_Y, dtype=torch.long)\n",
    "    output = torch.zeros(len(Y), max_len_Y, dtype=torch.long)\n",
    "\n",
    "    for i, s in enumerate(X):\n",
    "        enc_input[i, :len(s)] = torch.tensor(s)\n",
    "\n",
    "    for i, s in enumerate(Y):\n",
    "        dec_input[i, :len(s)-1] = torch.tensor(s[:-1])\n",
    "        output[i, :len(s)-1] = torch.tensor(s[1:])\n",
    "\n",
    "    return enc_input, dec_input, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def batch_generator(X, Y, batch_size):\n",
    "    X, Y = shuffle(X, Y, random_state=42)\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        yield prepare_batch(X[i:i+batch_size], Y[i:i+batch_size])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "train_loader = batch_generator(src_train, tgt_train, batch_size)\n",
    "be, bd, bo = next(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 12]) torch.Size([4, 11]) torch.Size([4, 11])\n"
     ]
    }
   ],
   "source": [
    "print(be.size(), bd.size(), bo.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOS', 'I', \"'\", 'm', 'not', 'responsible', 'for', 'what', 'Tom', 'did', '.', 'EOS']\n",
      "['SOS', 'No', 'soy', 'responsable', 'de', 'lo', 'que', 'hizo', 'Tom', '.', 'PAD']\n",
      "['No', 'soy', 'responsable', 'de', 'lo', 'que', 'hizo', 'Tom', '.', 'EOS', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "print([src_i2w[w.item()] for w in be[0]])\n",
    "print([tgt_i2w[w.item()] for w in bd[0]])\n",
    "print([tgt_i2w[w.item()] for w in bo[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=config.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConfig(d_model=768, n_layer=6, n_head=8, d_ff=2048, src_vocab_size=14779, tgt_vocab_size=28993, drop=0.1, max_seq_len=278, pad_token_id=0, activation='gelu')"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.n_head = 8\n",
    "config.max_seq_len = max([len(s) for s in src + tgt])\n",
    "config.src_vocab_size = len(src_w2i)\n",
    "config.tgt_vocab_size = len(tgt_w2i)\n",
    "\n",
    "model = Transformer(config).to(device)\n",
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform a dummy run of 2 epochs with a batch of 3 to demonstrate that the model can be trained. We recommend at least 20 epochs and a batch of len(src_train) // batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: ['SOS', 'My', 'family', 'is', 'not', 'that', 'large', '.', 'EOS', 'PAD', 'PAD']\n",
      "tgt: ['SOS', 'Mi', 'familia', 'no', 'es', 'tan', 'grande', '.', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "out: ['subjetiva', 'localizar', 'durísimo', 'Dejad', 'boicot', 'orificio', 'movieran', 'Léelo', 'perspectivas', 'deshago', 'meticulosamente', 'colaborar']\n",
      "--\n",
      "src: ['SOS', 'She', 'pressured', 'him', 'to', 'quit', 'his', 'job', '.', 'EOS']\n",
      "tgt: ['SOS', 'Ella', 'le', 'presionó', 'para', 'que', 'dejase', 'su', 'trabajo', '.', 'PAD']\n",
      "out: ['Entonces', 'sigue', 'obsesionada', 'Damasco', 'lavados', 'descuida', 'quedármelo', 'versos', 'noviembre', 'subsistir', 'maravilloso']\n",
      "--\n",
      "src: ['SOS', 'Tom', 'crashed', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tgt: ['SOS', 'Tom', 'se', 'estrelló', '.', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "out: ['Sumatra', 'Apunta', 'afeitándote', 'Alcancé', 'desearía', 'afronte', 'constantemente', 'afronte', 'desearía']\n",
      "--\n",
      "----------------------------------------------------------\n",
      "Epoch: 0, Loss: 10.421916325887045\n",
      "----------------------------------------------------------\n",
      "src: ['SOS', 'Can', 'I', 'get', 'by', 'the', 'guard', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tgt: ['SOS', '¿', 'Podré', 'burlar', 'la', 'guardia', '?', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "out: ['Escribes', 'descubierto', 'políticamente', 'deposita', 'católica', 'Merecés', 'me', 'bombilla', 'ridiculizarlo', 'llevara', 'abre']\n",
      "--\n",
      "src: ['SOS', 'What', 'did', 'you', 'give', 'Tom', 'on', 'his', 'birthday', '?', 'EOS', 'PAD', 'PAD', 'PAD']\n",
      "tgt: ['SOS', '¿', 'Qué', 'le', 'regalaste', 'a', 'Tom', 'en', 'su', 'cumpleaños', '?', 'PAD', 'PAD', 'PAD']\n",
      "out: ['plegable', 'burbujas', 'Cancelé', 'titubees', 'Relajate', 'burles', 'empacando', 'elijas', 'galletitas', 'Beben', 'mountain', 'cazafantasmas', 'Salud', 'Halagar']\n",
      "--\n",
      "src: ['SOS', 'We', 'can', 'cure', 'some', 'types', 'of', 'cancer', '.', 'EOS', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "tgt: ['SOS', 'Podemos', 'curar', 'algunos', 'tipos', 'de', 'cáncer', '.', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n",
      "out: ['departamentos', 'pereza', 'deshago', 'almorzaba', 'lombriz', 'estúpida', 'Hablo', 'supremo', 'comprasteis', 'tomarías', 'Comí', 'cigarrillos', 'llevado', 'frescos', 'manuscrito', 'Bonaparte', 'déjalo']\n",
      "--\n",
      "----------------------------------------------------------\n",
      "Epoch: 1, Loss: 10.456007957458496\n",
      "----------------------------------------------------------\n",
      "Training finished\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = [] \n",
    "\n",
    "    for _ in range(3): # range(len(src_train) // batch_size):\n",
    "        be, bd, bs = next(train_loader)\n",
    "        be, bd, bs = be.to(device), bd.to(device), bs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(be, bd)\n",
    "        loss = criterion(output.permute(0, 2, 1), bs)\n",
    "        # src, tgt, output\n",
    "        print(f'src: {[src_i2w[w.item()] for w in be[0]]}')\n",
    "        print(f'tgt: {[tgt_i2w[w.item()] for w in bd[0]]}')\n",
    "        print(f'out: {[tgt_i2w[w.item()] for w in output.argmax(dim=-1)[0]]}')\n",
    "        print(\"--\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.item())\n",
    "    print(\"----------------------------------------------------------\")\n",
    "    print(f'Epoch: {epoch}, Loss: {sum(epoch_loss) / len(epoch_loss)}')\n",
    "    print(\"----------------------------------------------------------\")\n",
    "print('Training finished')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Transformer implementation in Pytorch based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Self-Attention\n",
    "\n",
    "The intuition behind ``self-attention`` is that averaging token embeddings instead of using a fixed embedding for each token, enables the model to capture how words relate to each other in the input. In practice, said weighted relationships (attention weights) represent the syntactic and contextual structure of the sentence, leading to a more nuanced and rich understanding of the data.\n",
    "\n",
    "The most common way to implement a self-attention layer is based on ``scaled dot-product attention``, which involves:\n",
    "1. ``Linear projection`` of each token embedding into three vectors: ``query (q)``, ``key (k)``, ``value (v)``.\n",
    "2. Compute ``scaled attention scores``: we determine the similary between ``q`` and ``k`` by applying the ``dot product``. Since the results of this function are typically large numbers, they are then divided by a scaling factor inferred from the dimensionality of (k). This scaling contributes to stabilize gradients during training.\n",
    "3. Normalize the ``attention scores`` into ``attention weights`` by applying softmax (this ensures all the values sum to 1).\n",
    "4. ``Update the token embeddings`` by multiplying the attention weights by the value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single attention head within a multi-head attention mechanism.\n",
    "    \n",
    "    Parameters:\n",
    "    embed_dim (int): The size of the input feature dimension.\n",
    "    head_dim (int): The size of the output feature dimension for this attention head.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        # step_1: linear projections to query (q), key (k), and value (v) vectors\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Computes the scaled dot-product attention.\n",
    "\n",
    "        Parameters:\n",
    "        q, k, v (torch.Tensor): Query, Key, and Value tensors.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output after applying attention mechanism.\n",
    "        \"\"\"\n",
    "        dim_k = torch.tensor(k.size(-1), dtype=torch.float32)\n",
    "        # step_2: calculate similarity with the dot product, and scale attention scores\n",
    "        scaled_attention_scores = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(dim_k)\n",
    "        # step_3: normalize the attention scores with the softmax function\n",
    "        attention_weights = torch.softmax(scaled_attention_scores, axis=-1)\n",
    "        # step_4: update the token embeddings by multiplying attention weights by the value vector\n",
    "        output = torch.bmm(attention_weights, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the AttentionHead.\n",
    "\n",
    "        Parameters:\n",
    "        hidden_state (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output tensor after attention is applied.\n",
    "        \"\"\"\n",
    "        attn_outputs = self.scaled_dot_product_attention(self.q(hidden_state),\n",
    "                                                         self.k(hidden_state), \n",
    "                                                         self.v(hidden_state))\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-Headed Attention\n",
    "\n",
    "In a standard attention mechanism, the ``softmax`` of a single head tends to concentrate on a specific aspect of similarity, potentially overlooking other relevant features in the input. By integrating multiple attention heads, the model gains the ability to simultaneously attend to various aspects of the input data.\n",
    "\n",
    "The basic approach to implement Multi-Headed Attention comprises:\n",
    "\n",
    "1. Initialize the ``attention heads``. E.g. BERT has 12 attention heads whereas the embeddings dimension is 768, resulting in 768 / 12 = 64 as the head dimension.\n",
    "2. ``Concatenate attention heads`` to combines the outputs of the attention heads into a single vector while preserving the dimensionality of the embeddings.\n",
    "3. Apply a ``linear projection``.\n",
    "\n",
    "> Note that the softmax function is a probability distribution, which when applied within a single attention head tends to amplify certain features (those with higher scores) while diminishing others. Thus, leading to a focus on specific aspects of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Attention mechanism.\n",
    "\n",
    "    Parameters:\n",
    "    embed_dim (int): The size of the input feature dimension.\n",
    "    num_heads (int): The number of attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        if embed_dim < 0 or num_heads < 0:\n",
    "            raise ValueError(\"Embedding dimension and number of heads must be greater than 0\")\n",
    "        \n",
    "        super().__init__()\n",
    "        # step_1: initialize the attention heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "       \n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MultiHeadAttention.\n",
    "\n",
    "        Parameters:\n",
    "        hidden_state (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output tensor after multi-head attention is applied.\n",
    "        \"\"\"\n",
    "        # step_2: concatenate attention heads\n",
    "        attn_outputs = torch.cat([head(hidden_state) for head in self.heads], dim=-1)\n",
    "        # step_3: apply linear projection\n",
    "        outputs = self.output_linear(attn_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn = MultiHeadAttention(embed_dim=768, num_heads=12)\n",
    "attn_outputs = multihead_attn(torch.rand(1, 10, 768))\n",
    "attn_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Position-Wise Feed-Forward Layer\n",
    "\n",
    "The Transformer, primarily built upon linear operations like ``dot products`` and ``linear projections``, relies on the ``Position-Wise Feed-Forward Layer`` to introduce non-linearity into the model. This non-linearity enables the model to capture complex data patterns and relationships. The layer typically consists of two linear transformations with a ``non-linear activation function (like ReLU or GELU)``. Each layer in the ``Encoder`` and ``Decoder`` includes one of these feed-forward networks, allowing the model to build increasingly abstract representations of the input data as it passes through successive layers. Note that since this layer processes each embedding independly, the computations can be fully parallelized.\n",
    "\n",
    "In summary, this Layer comprises:\n",
    "- ``First linear transformation`` to the input tensor. \n",
    "- A non-linear ``activation function`` to allow the model learn more complex patterns.\n",
    "- ``Second linear transformation``, increasing the model's capacity to learn complex relationships in the data.\n",
    "- ``Dropout``, a regularization technique used to prevent overfitting. It randomly zeroes some of the elements of the input tensor with a certain probability during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the PositionWiseFeedForward layer.\n",
    "\n",
    "    Parameters:\n",
    "    embed_dim (int): The size of the input feature dimension.\n",
    "    ff_dim (int): The size of the hidden layer dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear_2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the PositionWiseFeedForward layer.\n",
    "\n",
    "        Parameters:\n",
    "        hidden_state (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output tensor after applying the feed-forward network.\n",
    "        \"\"\"\n",
    "        return self.dropout(0.1)(self.linear_2(self.activation(self.linear_1(hidden_state))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward = PositionWiseFeedForward(embed_dim=768, ff_dim=3072)\n",
    "feed_forward_outputs = feed_forward(torch.rand(1, 10, 768))\n",
    "feed_forward_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Positional Embeddings\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decoder\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Encoder\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

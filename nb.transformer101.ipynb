{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Transformer implementation in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Self-Attention\n",
    "\n",
    "The intuition behind ``self-attention`` is that averaging token embeddings instead of using a fixed embedding for each token, enables the model to capture how words relate to each other in the input. In practice, said weighted relationships (attention weights) represent the syntactic and contextual structure of the sentence, leading to a more nuanced and rich understanding of the data.\n",
    "\n",
    "The most common way to implement a self-attention layer is based on ``scaled dot-product attention``, which involves:\n",
    "1. ``Linear projection`` of each token embedding into three vectors: ``query (q)``, ``key (k)``, ``value (v)``.\n",
    "2. Compute ``scaled attention scores``: we determine the similary between ``q`` and ``k`` by applying the dot product. Since the results of this function are typically large numbers, they are then divided by a scaling factor inferred from the dimensionality of (k). This scaling is used to stabilize gradients during training.\n",
    "3. Normalize the ``attention scores`` into ``attention weights`` by applying softmax (this ensures all the values sum to 1).\n",
    "4. ``Update the token embeddings`` by multiplying the attention weights by the value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Represents a single attention head within a multi-head attention mechanism.\n",
    "    \n",
    "    Parameters:\n",
    "    embed_dim (int): The size of the input feature dimension.\n",
    "    head_dim (int): The size of the output feature dimension for this attention head.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 1: linear projection of tokens embeddings into query, key, and value vectors\n",
    "        # nn.Linear(in_features, out_features, bias=True) creates a linear transformation\n",
    "        # wherein each input vector of size embed_dim will be transformed into a \n",
    "        # lower-dimensional vector of size head_dim.\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v):\n",
    "        \"\"\"\n",
    "        Computes the scaled dot-product attention.\n",
    "\n",
    "        Parameters:\n",
    "        q, k, v (torch.Tensor): Query, Key, and Value tensors.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: Output after applying attention mechanism.\n",
    "        \"\"\"\n",
    "        dim_k = torch.tensor(k.size(-1), dtype=torch.float32)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 2: calculate the scaled attention scores\n",
    "        # torch.bmm performs a batch matrix-matrix product of q and k.\n",
    "        # we then apply the scaling factor 1/sqrt(k_dim) to said dot product.\n",
    "        scaled_attention_scores = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(dim_k)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 3: apply a softmax function to obtain the attention weights\n",
    "        attention_weights = torch.softmax(scaled_attention_scores, axis=-1)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 4: update tokens embeddings by applying attention weights to the value vector\n",
    "        output = torch.bmm(attention_weights, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the AttentionHead.\n",
    "\n",
    "        Parameters:\n",
    "        hidden_state (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output tensor after attention is applied.\n",
    "        \"\"\"\n",
    "        attn_outputs = self.scaled_dot_product_attention(self.q(hidden_state),\n",
    "                                                         self.k(hidden_state), \n",
    "                                                         self.v(hidden_state))\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multi-Headed Attention\n",
    "\n",
    "In a standard attention mechanism, the softmax of a single head tends to concentrate on a specific aspect of similarity, potentially overlooking other relevant features in the input. By integrating multiple attention heads, the model gains the ability to simultaneously attend to various aspects of the input data such as:\n",
    "- semantic meaning of words\n",
    "- grammatical relationships\n",
    "- tone or sentiment\n",
    "- intended modality\n",
    "- idiomatic expressions\n",
    "- [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the Multi-Head Attention mechanism.\n",
    "\n",
    "    Parameters:\n",
    "    embed_dim (int): The size of the input feature dimension.\n",
    "    num_heads (int): The number of attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 1: initialize the attention heads\n",
    "        # E.g. BERT has 12 attention heads whereas the embeddings dimension is 768 \n",
    "        # resulting in 768 / 12 = 64 as the head dimension\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n",
    "        head_dim = embed_dim // num_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 2: prepare linear transformation\n",
    "        # combines the outputs of the attention heads into a single vector while preserving\n",
    "        # the dimensionality of the embeddings\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MultiHeadAttention.\n",
    "\n",
    "        Parameters:\n",
    "        hidden_state (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output tensor after multi-head attention is applied.\n",
    "        \"\"\"\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 3: concatenate attention heads\n",
    "        attn_outputs = torch.cat([head(hidden_state) for head in self.heads], dim=-1)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 4: linear projection of concatenated attention heads\n",
    "        outputs = self.output_linear(attn_outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihead_attn = MultiHeadAttention(embed_dim=768, num_heads=12)\n",
    "attn_outputs = multihead_attn(torch.rand(1, 10, 768))\n",
    "attn_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Position-Wise Feed-Forward Layer\n",
    "\n",
    "The Transformer, primarily built upon linear operations like dot products and linear projections, relies on the Position-Wise Feed-Forward Layer to introduce non-linearity into the model. This non-linearity enables the model to capture complex data patterns and relationships. The layer typically consists of two linear transformations with a non-linear activation function (like ReLU or GELU). Each layer in the Encoder and Decoder includes one of these feed-forward networks, allowing the model to build increasingly abstract representations of the input data as it passes through successive layers.\n",
    "\n",
    "Note that since this layer processes each embedding independly, the computations can be fully parallelized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the PositionWiseFeedForward layer.\n",
    "\n",
    "    Parameters:\n",
    "    embed_dim (int): The size of the input feature dimension.\n",
    "    ff_dim (int): The size of the hidden layer dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.linear_2 = nn.Linear(ff_dim, embed_dim)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the PositionWiseFeedForward layer.\n",
    "\n",
    "        Parameters:\n",
    "        x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The output tensor after applying the feed-forward network.\n",
    "        \"\"\"\n",
    "        # This layer applies a linear transformation to the input tensor. \n",
    "        # It's a fully connected layer where each input is connected to every output by a learned weight.\n",
    "        x = self.linear_1(hidden_state)\n",
    "        # This step introduces non-linearity into the model, allowing it to learn more complex patterns.\n",
    "        x = self.activation(x)\n",
    "        # This layer applies another linear transformation to the output of the previous layer,\n",
    "        # potentially increasing the model's capacity to learn complex relationships in the data.\n",
    "        x = self.linear_2(x)\n",
    "        # Dropout is a regularization technique used to prevent overfitting. It randomly zeroes \n",
    "        # some of the elements of the input tensor with a certain probability during training.\n",
    "        return nn.Dropout(0.1)(x)\n",
    "\n",
    "        # return self.dropout(self.linear_2(self.activation(self.linear_1(hidden_state))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward = PositionWiseFeedForward(embed_dim=768, ff_dim=3072)\n",
    "feed_forward_outputs = feed_forward(torch.rand(1, 10, 768))\n",
    "feed_forward_outputs.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Positional Embeddings\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Decoder\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Encoder\n",
    "\n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

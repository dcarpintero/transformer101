{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Transformer implementation in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Self-Attention\n",
    "\n",
    "The intuition behind ``self-attention`` is that averaging token embeddings instead of using a fixed embedding for each token, enables the model to capture how words relate to each other in the input. In practice, said weighted relationships (attention weights) represent the syntactic and contextual structure of the sentence, leading to a more nuanced and rich understanding of the data.\n",
    "\n",
    "The most common way to implement a self-attention layer is based on ``scaled dot-product attention``, which involves:\n",
    "1. ``Linear projection`` of each token embedding into three vectors: ``query (q)``, ``key (k)``, ``value (v)``.\n",
    "2. Compute ``scaled attention scores``: we determine the similary between ``q`` and ``k`` by applying the dot product. Since the results of this function are typically large numbers, they are then multiplied by a scaling factor inferred from the dimensionality of (k).\n",
    "3. Normalize the ``attention scores`` into ``attention weights`` by applying softmax (this ensures all the values sum to 1).\n",
    "4. ``Update the token embeddings`` by multiplying the attention weights by the value vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        super().__init__()\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 1: linear projection of tokens embeddings into query, key, and value vectors\n",
    "        # nn.Linear(in_features, out_features, bias=True) creates a linear transformation.\n",
    "        # self.x is an instance of nn.Linear, this allows the transformation to be applied \n",
    "        # by calling self.x(input).\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "\n",
    "    def scaled_dot_product_attention_1(q, k, v):\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 2: calculate the scaled attention scores\n",
    "        # torch.bmm performs a batch matrix-matrix product of q and k.\n",
    "        # we then apply the scaling factor 1/sqrt(k_dim) to said dot product.\n",
    "        scaled_attention_scores = torch.bmm(q, k.transpose(1, 2)) / torch.sqrt(k.size(-1))\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 3: apply a softmax function to obtain the attention weights\n",
    "        attention_weights = torch.softmax(scaled_attention_scores, axis=-1)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 4: update tokens embeddings by applying attention weights to the value vector\n",
    "        output = torch.bmm(attention_weights, v)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_outputs = self.scaled_dot_product_attention(self.q(x), self.k(x), self.v(x))\n",
    "        return attn_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 1: initialize the attention heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(embed_dim, head_dim) for _ in range(num_heads)])\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 2: linear projection of concatenated attention heads\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 3: concatenate attention heads\n",
    "        attn_outputs = torch.cat([head(hidden_state) for head in self.heads], dim=-1)\n",
    "        # ----------------------------------------------------------------------------------\n",
    "        # step 4: linear projection of concatenated attention heads\n",
    "        outputs = self.output_linear(attn_outputs)\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
